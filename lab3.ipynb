{"cells":[{"cell_type":"markdown","metadata":{"id":"gxetfiZO85PV"},"source":["# Lab 3: Bayes Classifier and Boosting"]},{"cell_type":"markdown","metadata":{"id":"jzTexyBL85PZ"},"source":["## Jupyter notebooks\n","\n","In this lab, you can use Jupyter <https://jupyter.org/> to get a nice layout of your code and plots in one document. However, you may also use Python as usual, without Jupyter.\n","\n","If you have Python and pip, you can install Jupyter with `sudo pip install jupyter`. Otherwise you can follow the instruction on <http://jupyter.readthedocs.org/en/latest/install.html>.\n","\n","And that is everything you need! Now use a terminal to go into the folder with the provided lab files. Then run `jupyter notebook` to start a session in that folder. Click `lab3.ipynb` in the browser window that appeared to start this very notebook. You should click on the cells in order and either press `ctrl+enter` or `run cell` in the toolbar above to evaluate all the expressions.\n","\n","Be sure to put `%matplotlib inline` at the top of every code cell where you call plotting functions to get the resulting plots inside the document."]},{"cell_type":"markdown","metadata":{"id":"Q_I9nq-R85Pa"},"source":["## Import the libraries\n","\n","In Jupyter, select the cell below and press `ctrl + enter` to import the needed libraries.\n","Check out `labfuns.py` if you are interested in the details."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"OurF1OCD85Pb","executionInfo":{"status":"error","timestamp":1665306366570,"user_tz":-120,"elapsed":2266,"user":{"displayName":"Carlos Gil","userId":"03537498225155186349"}},"outputId":"994c63c0-ce9e-4a5a-a085-1a1d3f4d2253"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-07bd8fec77c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab_Notebooks/LAB3_ML'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mephemeral\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       readonly=readonly)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mnormed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must be in a directory that exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m   \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_signal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGKILL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Mountpoint must be in a directory that exists"]}],"source":["import numpy as np\n","from scipy import misc\n","from imp import reload\n","from labfuns import *\n","import random"]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_FPENmcI-96r","executionInfo":{"status":"ok","timestamp":1665306195055,"user_tz":-120,"elapsed":315,"user":{"displayName":"Carlos Gil","userId":"03537498225155186349"}},"outputId":"e12474fa-d70c-4858-95d0-8500af27a1da"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"markdown","metadata":{"id":"TC4gWiJO85Pc"},"source":["## Bayes classifier functions to implement\n","\n","The lab descriptions state what each function should do."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"J2rNwioO85Pc"},"outputs":[],"source":["# NOTE: you do not need to handle the W argument for this part!\n","# in: labels - N vector of class labels\n","# out: prior - C x 1 vector of class priors\n","def computePrior(labels, W=None):\n","    Npts = labels.shape[0]\n","    if W is None:\n","        W = np.ones((Npts,1))/Npts\n","    else:\n","        assert(W.shape[0] == Npts)\n","    classes = np.unique(labels)\n","    Nclasses = np.size(classes)\n","\n","    prior = np.zeros((Nclasses,1))\n","\n","    # TODO: compute the values of prior for each class!\n","    # ==========================\n","    \n","    # ==========================\n","\n","    return prior\n","\n","# NOTE: you do not need to handle the W argument for this part!\n","# in:      X - N x d matrix of N data points\n","#     labels - N vector of class labels\n","# out:    mu - C x d matrix of class means (mu[i] - class i mean)\n","#      sigma - C x d x d matrix of class covariances (sigma[i] - class i sigma)\n","def mlParams(X, labels, W=None):\n","    assert(X.shape[0]==labels.shape[0])\n","    Npts,Ndims = np.shape(X)\n","    classes = np.unique(labels)\n","    Nclasses = np.size(classes)\n","\n","    if W is None:\n","        W = np.ones((Npts,1))/float(Npts)\n","\n","    mu = np.zeros((Nclasses,Ndims))\n","    sigma = np.zeros((Nclasses,Ndims,Ndims))\n","\n","    # TODO: fill in the code to compute mu and sigma!\n","    # ==========================\n","    dummy = []\n","    for jdx,class in enumerate(classes):\n","      idx = y==class # Returns a true or false with the length of y\n","      # Or more compactly extract the indices for which y==class is true,\n","      # analogous to MATLABâ€™s find\n","      idx = np.where(y==class)[0]\n","      xlc = X[idx,:] # Get the x for the class labels. Vectors are rows.\n","      dummy = xlc\n","    # ==========================\n","\n","    return mu, sigma, dummy\n","\n","# in:      X - N x d matrix of M data points\n","#      prior - C x 1 matrix of class priors\n","#         mu - C x d matrix of class means (mu[i] - class i mean)\n","#      sigma - C x d x d matrix of class covariances (sigma[i] - class i sigma)\n","# out:     h - N vector of class predictions for test points\n","def classifyBayes(X, prior, mu, sigma):\n","\n","    Npts = X.shape[0]\n","    Nclasses,Ndims = np.shape(mu)\n","    logProb = np.zeros((Nclasses, Npts))\n","\n","    # TODO: fill in the code to compute the log posterior logProb!\n","    # ==========================\n","    \n","    # ==========================\n","    \n","    # one possible way of finding max a-posteriori once\n","    # you have computed the log posterior\n","    h = np.argmax(logProb,axis=0)\n","    return h"]},{"cell_type":"code","source":["np.random.seed(10)\n","\n","X = np.random.randn(100,10)\n","y = np.array([])\n","\n","for _ in range(X.shape[0])\n","  y[_] = np.random.randint(1,5)\n","\n","a, b, c = mlParams(X, y)\n","\n","print(c, c.shape)\n"],"metadata":{"id":"ljyXO1nD9OgS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tMlQNiQd85Pd"},"source":["The implemented functions can now be summarized into the `BayesClassifier` class, which we will use later to test the classifier, no need to add anything else here:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"zIeaEGnO85Pd"},"outputs":[],"source":["# NOTE: no need to touch this\n","class BayesClassifier(object):\n","    def __init__(self):\n","        self.trained = False\n","\n","    def trainClassifier(self, X, labels, W=None):\n","        rtn = BayesClassifier()\n","        rtn.prior = computePrior(labels, W)\n","        rtn.mu, rtn.sigma = mlParams(X, labels, W)\n","        rtn.trained = True\n","        return rtn\n","\n","    def classify(self, X):\n","        return classifyBayes(X, self.prior, self.mu, self.sigma)"]},{"cell_type":"markdown","metadata":{"id":"fAnOWZCo85Pe"},"source":["## Test the Maximum Likelihood estimates\n","\n","Call `genBlobs` and `plotGaussian` to verify your estimates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6mm2_DV85Pe"},"outputs":[],"source":["%matplotlib inline\n","\n","X, labels = genBlobs(centers=5)\n","mu, sigma = mlParams(X,labels)\n","plotGaussian(X,labels,mu,sigma)"]},{"cell_type":"markdown","metadata":{"id":"zU20BhNP85Pe"},"source":["Call the `testClassifier` and `plotBoundary` functions for this part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDaVYdnQ85Pf"},"outputs":[],"source":["testClassifier(BayesClassifier(), dataset='iris', split=0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CG1YBFMw85Pf"},"outputs":[],"source":["testClassifier(BayesClassifier(), dataset='vowel', split=0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ygbElq3J85Pf"},"outputs":[],"source":["%matplotlib inline\n","plotBoundary(BayesClassifier(), dataset='iris',split=0.7)"]},{"cell_type":"markdown","metadata":{"id":"K5y4S81F85Pf"},"source":["## Boosting functions to implement\n","\n","The lab descriptions state what each function should do."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"kX-h5UYE85Pf"},"outputs":[],"source":["# in: base_classifier - a classifier of the type that we will boost, e.g. BayesClassifier\n","#                   X - N x d matrix of N data points\n","#              labels - N vector of class labels\n","#                   T - number of boosting iterations\n","# out:    classifiers - (maximum) length T Python list of trained classifiers\n","#              alphas - (maximum) length T Python list of vote weights\n","def trainBoost(base_classifier, X, labels, T=10):\n","    # these will come in handy later on\n","    Npts,Ndims = np.shape(X)\n","\n","    classifiers = [] # append new classifiers to this list\n","    alphas = [] # append the vote weight of the classifiers to this list\n","\n","    # The weights for the first iteration\n","    wCur = np.ones((Npts,1))/float(Npts)\n","\n","    for i_iter in range(0, T):\n","        # a new classifier can be trained like this, given the current weights\n","        classifiers.append(base_classifier.trainClassifier(X, labels, wCur))\n","\n","        # do classification for each point\n","        vote = classifiers[-1].classify(X)\n","\n","        # TODO: Fill in the rest, construct the alphas etc.\n","        # ==========================\n","        \n","        # alphas.append(alpha) # you will need to append the new alpha\n","        # ==========================\n","        \n","    return classifiers, alphas\n","\n","# in:       X - N x d matrix of N data points\n","# classifiers - (maximum) length T Python list of trained classifiers as above\n","#      alphas - (maximum) length T Python list of vote weights\n","#    Nclasses - the number of different classes\n","# out:  yPred - N vector of class predictions for test points\n","def classifyBoost(X, classifiers, alphas, Nclasses):\n","    Npts = X.shape[0]\n","    Ncomps = len(classifiers)\n","\n","    # if we only have one classifier, we may just classify directly\n","    if Ncomps == 1:\n","        return classifiers[0].classify(X)\n","    else:\n","        votes = np.zeros((Npts,Nclasses))\n","\n","        # TODO: implement classificiation when we have trained several classifiers!\n","        # here we can do it by filling in the votes vector with weighted votes\n","        # ==========================\n","        \n","        # ==========================\n","\n","        # one way to compute yPred after accumulating the votes\n","        return np.argmax(votes,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"GCM5RCFG85Pg"},"source":["The implemented functions can now be summarized another classifer, the `BoostClassifier` class. This class enables boosting different types of classifiers by initializing it with the `base_classifier` argument. No need to add anything here."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"1J_ZnOHV85Pg"},"outputs":[],"source":["# NOTE: no need to touch this\n","class BoostClassifier(object):\n","    def __init__(self, base_classifier, T=10):\n","        self.base_classifier = base_classifier\n","        self.T = T\n","        self.trained = False\n","\n","    def trainClassifier(self, X, labels):\n","        rtn = BoostClassifier(self.base_classifier, self.T)\n","        rtn.nbr_classes = np.size(np.unique(labels))\n","        rtn.classifiers, rtn.alphas = trainBoost(self.base_classifier, X, labels, self.T)\n","        rtn.trained = True\n","        return rtn\n","\n","    def classify(self, X):\n","        return classifyBoost(X, self.classifiers, self.alphas, self.nbr_classes)"]},{"cell_type":"markdown","metadata":{"id":"69fDHEw_85Pg"},"source":["## Run some experiments\n","\n","Call the `testClassifier` and `plotBoundary` functions for this part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UoRiGOYw85Pg"},"outputs":[],"source":["testClassifier(BoostClassifier(BayesClassifier(), T=10), dataset='iris',split=0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UB2i6XZs85Pg"},"outputs":[],"source":["testClassifier(BoostClassifier(BayesClassifier(), T=10), dataset='vowel',split=0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EhC5NBOW85Ph"},"outputs":[],"source":["%matplotlib inline\n","plotBoundary(BoostClassifier(BayesClassifier()), dataset='iris',split=0.7)"]},{"cell_type":"markdown","metadata":{"id":"WEVLtMSI85Ph"},"source":["Now repeat the steps with a decision tree classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfcC-A_O85Ph"},"outputs":[],"source":["testClassifier(DecisionTreeClassifier(), dataset='iris', split=0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JAgvLd6u85Ph"},"outputs":[],"source":["testClassifier(BoostClassifier(DecisionTreeClassifier(), T=10), dataset='iris',split=0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_iSSOzd85Ph"},"outputs":[],"source":["testClassifier(DecisionTreeClassifier(), dataset='vowel',split=0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YTiJwi9185Ph"},"outputs":[],"source":["testClassifier(BoostClassifier(DecisionTreeClassifier(), T=10), dataset='vowel',split=0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O3SvBEWD85Ph"},"outputs":[],"source":["%matplotlib inline\n","plotBoundary(DecisionTreeClassifier(), dataset='iris',split=0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_fCIXPVd85Ph"},"outputs":[],"source":["%matplotlib inline\n","plotBoundary(BoostClassifier(DecisionTreeClassifier(), T=10), dataset='iris',split=0.7)"]},{"cell_type":"markdown","metadata":{"id":"uVLz6qFR85Pi"},"source":["## Bonus: Visualize faces classified using boosted decision trees\n","\n","Note that this part of the assignment is completely voluntary! First, let's check how a boosted decision tree classifier performs on the olivetti data. Note that we need to reduce the dimension a bit using PCA, as the original dimension of the image vectors is `64 x 64 = 4096` elements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tNpEt1385Pi"},"outputs":[],"source":["testClassifier(BayesClassifier(), dataset='olivetti',split=0.7, dim=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JL3oJYOn85Pi"},"outputs":[],"source":["testClassifier(BoostClassifier(DecisionTreeClassifier(), T=10), dataset='olivetti',split=0.7, dim=20)"]},{"cell_type":"markdown","metadata":{"id":"egBPP83e85Pi"},"source":["You should get an accuracy around 70%. If you wish, you can compare this with using pure decision trees or a boosted bayes classifier. Not too bad, now let's try and classify a face as belonging to one of 40 persons!"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"MmAcVDsl85Pi"},"outputs":[],"source":["%matplotlib inline\n","X,y,pcadim = fetchDataset('olivetti') # fetch the olivetti data\n","xTr,yTr,xTe,yTe,trIdx,teIdx = trteSplitEven(X,y,0.7) # split into training and testing\n","pca = decomposition.PCA(n_components=20) # use PCA to reduce the dimension to 20\n","pca.fit(xTr) # use training data to fit the transform\n","xTrpca = pca.transform(xTr) # apply on training data\n","xTepca = pca.transform(xTe) # apply on test data\n","# use our pre-defined decision tree classifier together with the implemented\n","# boosting to classify data points in the training data\n","classifier = BoostClassifier(DecisionTreeClassifier(), T=10).trainClassifier(xTrpca, yTr)\n","yPr = classifier.classify(xTepca)\n","# choose a test point to visualize\n","testind = random.randint(0, xTe.shape[0]-1)\n","# visualize the test point together with the training points used to train\n","# the class that the test point was classified to belong to\n","visualizeOlivettiVectors(xTr[yTr == yPr[testind],:], xTe[testind,:])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}